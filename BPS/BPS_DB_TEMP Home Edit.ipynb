{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFunction \"createTable\" executed\n",
      "\tFunction \"makeCreateTableQuery\" executed\n",
      "CREATE TABLE BPS_NODELIST(        IP VARCHAR2 (100) NOT NULL,         PORT VARCHAR2 (100) NOT NULL,         CONNECTION_FAIL NUMBER NOT NULL         )\n"
     ]
    }
   ],
   "source": [
    "createTable(db_nodeListTableName, db_nodeListTableColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started httpserver on port  8099\n",
      "\tFunction \"initSvr\" executed\n",
      "\tFunction \"selectTable\" executed\n",
      "\tFunction \"selectTable\" executed\n",
      "Table select error, There are no table named \"BPS_BLOCK\" in db. \n",
      " It will be created\n",
      "\tFunction \"createTable\" executed\n",
      "\tFunction \"makeCreateTableQuery\" executed\n",
      "CREATE TABLE BPS_BLOCK(        BLOCKINDEX NUMBER NOT NULL,         PREVIOUSHASH VARCHAR2 (100) NOT NULL,         TIMESTAMP VARCHAR2 (100) NOT NULL,         DATA VARCHAR2 (100) NOT NULL,         CURRENTHASH VARCHAR2 (100) NOT NULL,         PROOF NUMBER NOT NULL,         CONSTRAINTS PK_BPS_BLOCK PRIMARY KEY(BLOCKINDEX)         )\n",
      "^C received, shutting down the web server\n",
      "(cx_Oracle.DatabaseError) ORA-00942: table or view does not exist [SQL: 'SELECT * FROM BPS_BLOCK'] (Background on this error at: http://sqlalche.me/e/4xp6)\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "from socketserver import ThreadingMixIn\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import parse_qs\n",
    "from urllib.parse import urlparse\n",
    "import threading\n",
    "import cgi\n",
    "import uuid\n",
    "from tempfile import NamedTemporaryFile\n",
    "import shutil\n",
    "import requests # for sending new block to other nodes\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, types\n",
    "import cx_Oracle as oci # for connect Oracle Database\n",
    "\n",
    "\n",
    "PORT_NUMBER = 8099\n",
    "db_ip = '192.168.110.3'\n",
    "db_port = '1522'\n",
    "db_serviceName = 'xe'\n",
    "db_id = 'DJ2019'\n",
    "db_pw = 'DJ2019'\n",
    "db_userTableName = 'BPS_USERS'\n",
    "db_userTableColumns = ('NAME', 'KEY', 'BALANCE')\n",
    "db_blockTableName = 'BPS_BLOCK'\n",
    "db_blockTableColumns = ('BLOCKINDEX', 'PREVIOUSHASH', 'TIMESTAMP', 'DATA', 'CURRENTHASH', 'PROOF')\n",
    "db_txTableName = 'BPS_TXDATA'\n",
    "db_txTableColumns = ('COMMIT_YN', 'SENDER', 'AMOUNT', 'RECEIVER', 'UUID', 'TX_VALIDITY')\n",
    "db_nodeListTableName = 'BPS_NODELIST'\n",
    "db_nodeListTableColumns = ('IP', 'PORT', 'CONNECTION_FAIL')\n",
    "g_receiveNewBlock = \"/node/receiveNewBlock\"\n",
    "g_difficulty = 4\n",
    "g_maximumTry = 100\n",
    "g_nodeList = {'trustedServerAddress':'8099'}\n",
    "\n",
    "class Block:\n",
    "\n",
    "    def __init__(self, index, previousHash, timestamp, data, currentHash, proof):\n",
    "        self.index = index\n",
    "        self.previousHash = previousHash\n",
    "        self.timestamp = timestamp\n",
    "        self.data = data\n",
    "        self.currentHash = currentHash\n",
    "        self.proof = proof\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n",
    "\n",
    "\n",
    "class txData:\n",
    "\n",
    "    def __init__(self, commitYN, sender, amount, receiver, uuid):\n",
    "        self.commitYN = commitYN\n",
    "        self.sender = sender\n",
    "        self.amount = amount\n",
    "        self.receiver = receiver\n",
    "        self.uuid = uuid\n",
    "\n",
    "\n",
    "def makeCreateTableQuery(tableName, columns):\n",
    "    print('\\tFunction \"makeCreateTableQuery\" executed')\n",
    "\n",
    "    if (tableName == 'BPS_BLOCK'):\n",
    "        createTableQuery = \"CREATE TABLE BPS_BLOCK(\\\n",
    "        %s NUMBER NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s NUMBER NOT NULL, \\\n",
    "        CONSTRAINTS PK_BPS_BLOCK PRIMARY KEY(BLOCKINDEX) \\\n",
    "        )\" % columns\n",
    "\n",
    "    if (tableName == 'BPS_TXDATA'):\n",
    "        createTableQuery = \"CREATE TABLE BPS_TXDATA(\\\n",
    "        %s NUMBER NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s NUMBER NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s NUMBER NOT NULL, \\\n",
    "        CONSTRAINTS PK_BPS_BLOCK PRIMARY KEY(UUID) \\\n",
    "        )\" % columns\n",
    "\n",
    "    if (tableName == 'BPS_NODELIST'):\n",
    "        createTableQuery = \"CREATE TABLE BPS_NODELIST(\\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s NUMBER NOT NULL \\\n",
    "        )\" % columns\n",
    "\n",
    "    return createTableQuery\n",
    "\n",
    "def makeDropTableQuery(tableName):\n",
    "    print('\\tFunction \"makeDropTableQuery\" executed')\n",
    "\n",
    "    dropTableQuery = 'DROP TABLE %s' % tableName\n",
    "\n",
    "    return dropTableQuery\n",
    "\n",
    "def makeUpdateQuery(tableName, setValue, whereCondition):\n",
    "    print('\\tFunction \"makeUpdateQuery\" executed')\n",
    "    setValueInput = ''\n",
    "    for key, value in setValue.items():\n",
    "        setValueInput += str(key)\n",
    "        setValueInput += ' = '\n",
    "        setValueInput += str(value)\n",
    "        setValueInput += ', '\n",
    "    setValueInput = setValueInput.rstrip(', ')\n",
    "\n",
    "    whereConditionInput = ''\n",
    "    for key, value in setValue.items():\n",
    "        whereConditionInput += str(key)\n",
    "        whereConditionInput += ' = '\n",
    "        whereConditionInput += str(value)\n",
    "        whereConditionInput += ', '\n",
    "    whereConditionInput = whereConditionInput.rstrip(', ')\n",
    "    updateQuery = 'UPDATE %s SET %s WHERE %s' % (tableName, setValueInput, whereConditionInput)\n",
    "    return updateQuery\n",
    "\n",
    "def selectTable(tableName, columns, engine):\n",
    "    print('\\tFunction \"selectTable\" executed')\n",
    "    selectQuery = 'SELECT * FROM %s' % tableName\n",
    "    try:\n",
    "        resultData = pd.read_sql_query(selectQuery, engine)\n",
    "    except:\n",
    "        print('Table select error, There are no table named \"%s\" in db. \\n It will be created' % tableName)\n",
    "        createTable(tableName, columns)\n",
    "        resultData = pd.read_sql_query(selectQuery, engine)\n",
    "    return resultData\n",
    "\n",
    "\n",
    "def createTable(tableName, columns):\n",
    "    print('\\tFunction \"createTable\" executed')\n",
    "\n",
    "    connectComplete = False\n",
    "    cursorComplete = False\n",
    "\n",
    "    try:\n",
    "        connectInfo = db_id + '/' + db_pw + '@' + db_ip + ':' + db_port + '/' + db_serviceName\n",
    "        oracleConnect = oci.connect(connectInfo)\n",
    "        connectComplete = True\n",
    "        oracleCursor = oracleConnect.cursor()\n",
    "        cursorComplete = True\n",
    "\n",
    "        createTableQuery = makeCreateTableQuery(tableName, columns)\n",
    "        print(createTableQuery)\n",
    "        oracleCursor.execute(createTableQuery)\n",
    "        oracleConnect.commit()\n",
    "        oracleCursor.close()\n",
    "        oracleConnect.close\n",
    "\n",
    "    except:\n",
    "        if (cursorComplete == True):\n",
    "            oracleCursor.close()\n",
    "        if (connectComplete == True):\n",
    "            oracleConnect.close\n",
    "\n",
    "def replaceTable(tableName, columns):\n",
    "    print('\\tFunction \"replaceTable\" executed')\n",
    "    connectComplete = False\n",
    "    cursorComplete = False\n",
    "\n",
    "    try:\n",
    "        connectInfo = db_id + '/' + db_pw + '@' + db_ip + ':' + db_port + '/' + db_serviceName\n",
    "        oracleConnect = oci.connect(connectInfo)\n",
    "        connectComplete = True\n",
    "        oracleCursor = oracleConnect.cursor()\n",
    "        cursorComplete = True\n",
    "\n",
    "        dropTableQuery = makeDropTableQuery(tableName)\n",
    "        oracleCursor.execute(dropTableQuery)\n",
    "        createTableQuery = makeCreateTableQuery(tableName, columns)\n",
    "        oracleCursor.execute(createTableQuery)\n",
    "        oracleConnect.commit()\n",
    "        oracleCursor.close()\n",
    "        oracleConnect.close\n",
    "\n",
    "    except:\n",
    "        if (cursorComplete == True):\n",
    "            oracleCursor.close()\n",
    "        if (connectComplete == True):\n",
    "            oracleConnect.close\n",
    "\n",
    "\n",
    "def updateTable(tableName, setValue, whereCondition):\n",
    "    print('\\tFunction \"updateTable\" executed')\n",
    "\n",
    "    connectComplete = False\n",
    "    cursorComplete = False\n",
    "\n",
    "    try:\n",
    "        connectInfo = db_id + '/' + db_pw + '@' + db_ip + ':' + db_port + '/' + db_serviceName\n",
    "        oracleConnect = oci.connect(connectInfo)\n",
    "        connectComplete = True\n",
    "        oracleCursor = oracleConnect.cursor()\n",
    "        cursorComplete = True\n",
    "\n",
    "        updateQuery = makeUpdateQuery(tableName, setValue, whereCondition)\n",
    "        oracleCursor.execute(updateQuery)\n",
    "        oracleConnect.commit()\n",
    "        oracleCursor.close()\n",
    "        oracleConnect.close\n",
    "\n",
    "    except:\n",
    "        if (cursorComplete == True):\n",
    "            oracleCursor.close()\n",
    "        if (connectComplete == True):\n",
    "            oracleConnect.close\n",
    "\n",
    "def generateGenesisBlock(): #처음 블록을 생성 딱 한번만 호출\n",
    "    print('\\tFunction \"generateGenesisBlock\" executed')\n",
    "    timestamp = time.time()\n",
    "    print(\"time.time() => %f \\n\" % timestamp)\n",
    "    tempHash = calculateHash(0, '0', timestamp, \"Genesis Block\", 0)\n",
    "    print(tempHash)\n",
    "    return Block(0, '0', timestamp, \"Genesis Block\",  tempHash,0)\n",
    "\n",
    "def calculateHash(index, previousHash, timestamp, data, proof): #해쉬계산 블록번호, 이전블록 해쉬, 거래시간, 데이터, 작업증명을 넣어서 하고 16진수로 바꿈\n",
    "    print('\\tFunction \"calculateHash\" executed')\n",
    "    value = str(index) + str(previousHash) + str(timestamp) + str(data) + str(proof)\n",
    "    sha = hashlib.sha256(value.encode('utf-8'))\n",
    "    return str(sha.hexdigest())\n",
    "\n",
    "def calculateHashForBlock(block): #위에 있는 해쉬를 call\n",
    "    print('\\tFunction \"calculateHashForBlock\" executed')\n",
    "    return calculateHash(block.index, block.previousHash, block.timestamp, block.data, block.proof)\n",
    "\n",
    "def getLatestBlock(blockchain): #가장 최근의 블록\n",
    "    print('\\tFunction \"getLatestBlock\" executed')\n",
    "    return blockchain[len(blockchain) - 1]\n",
    "\n",
    "def generateNextBlock(blockchain, blockData, timestamp, proof): #다음블록생성\n",
    "    print('\\tFunction \"generateNextBlock\" executed')\n",
    "    previousBlock = getLatestBlock(blockchain)\n",
    "    nextIndex = int(previousBlock.index) + 1\n",
    "    nextTimestamp = timestamp\n",
    "    nextHash = calculateHash(nextIndex, previousBlock.currentHash, nextTimestamp, blockData, proof)\n",
    "    # index, previousHash, timestamp, data, currentHash, proof\n",
    "    return Block(nextIndex, previousBlock.currentHash, nextTimestamp, blockData, nextHash, proof)\n",
    "\n",
    "def writeBlockchain(blockchain):\n",
    "    print('\\tFunction \"writeBlockchain\" executed')\n",
    "    blockchainList = []\n",
    "    for block in blockchain:\n",
    "        blockList = [block.index, str(block.previousHash), str(block.timestamp), str(block.data),\n",
    "                     str(block.currentHash), block.proof]\n",
    "        blockchainList.append(blockList)\n",
    "\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "\n",
    "    blockReader = selectTable(db_blockTableName, db_blockTableColumns, engine)\n",
    "\n",
    "    lastLineNumber = len(blockReader)\n",
    "    for i in range(lastLineNumber):\n",
    "        lineNumber = i + 1\n",
    "        if (lineNumber == lastLineNumber):\n",
    "            line = blockReader.loc[i]\n",
    "            lastBlock = Block(line[0], line[1], line[2], line[3], line[4], line[5])\n",
    "\n",
    "    try:\n",
    "        if (lastBlock.index + 1 != int(blockchainList[-1][0]) or lastLineNumber + 1 != len(blockchainList)):\n",
    "            print(\"Index sequence mismatch\")\n",
    "            if (lastBlock.index == int(blockchainList[-1][0])):\n",
    "                print(\"DB has already been updated\")\n",
    "            return\n",
    "\n",
    "    except:\n",
    "        print(\n",
    "            'Index search error, There are no data or Existing table have problems. \\n It will be replaced by full data.')\n",
    "        pass\n",
    "\n",
    "    blockWriter = pd.DataFrame(blockchainList, columns=db_blockTableColumns)\n",
    "\n",
    "    # convert type to varchar if the types of the columns of a dataframe is object\n",
    "    replaceTable(db_blockTableName, db_blockTableColumns)\n",
    "    try:\n",
    "        to_varchar = {c: types.VARCHAR(blockWriter[c].str.len().max()) for c in\n",
    "                      blockWriter.columns[blockWriter.dtypes == 'object'].tolist()}\n",
    "        blockWriter.to_sql(db_blockTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "        print('Blockchain written to db')\n",
    "    except:\n",
    "        print('Data save error, It seems to have an integrity or type problem.')\n",
    "        to_varchar = {c: types.VARCHAR(blockReader[c].str.len().max()) for c in\n",
    "                      blockReader.columns[blockReader.dtypes == 'object'].tolist()}\n",
    "        blockReader.to_sql(db_blockTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "\n",
    "    # update txData cause it has been mined.\n",
    "    for block in blockchain:\n",
    "        updateTx(block)\n",
    "\n",
    "    print('Broadcasting new block to other nodes')\n",
    "    broadcastNewBlock(blockchain)\n",
    "\n",
    "\n",
    "def readBlockchain(tableName=db_blockTableName, columns=db_blockTableColumns, mode='internal'):\n",
    "    print('\\tFunction \"readBlockchain\" executed')\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "\n",
    "    print(\"readBlockChain\")\n",
    "    importedBlockchain = []\n",
    "\n",
    "    blockReader = selectTable(tableName, columns, engine)\n",
    "    try:\n",
    "        for i in range(len(blockReader)):\n",
    "            line = blockReader.loc[i]\n",
    "            block = Block(line[0], line[1], line[2], line[3], str(line[4]), str(line[5]))\n",
    "            importedBlockchain.append(block)\n",
    "        print(\"success pulling blockchain from DB\")\n",
    "        return importedBlockchain\n",
    "    except:\n",
    "        if mode == 'internal':\n",
    "            blockchain = generateGenesisBlock()\n",
    "            importedBlockchain.append(blockchain)\n",
    "            writeBlockchain(importedBlockchain)\n",
    "            return importedBlockchain\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def updateTx(blockData):\n",
    "    print('\\tFunction \"updateTx\" executed')\n",
    "    phrase = re.compile(\n",
    "        r\"\\w+[-]\\w+[-]\\w+[-]\\w+[-]\\w+\")  # [6b3b3c1e-858d-4e3b-b012-8faac98b49a8]UserID hwang sent 333 bitTokens to UserID kim.\n",
    "    matchList = phrase.findall(blockData.data)\n",
    "\n",
    "    if len(matchList) == 0:\n",
    "        print(\"No Match Found! \" + str(blockData.data) + \"block idx: \" + str(blockData.index))\n",
    "        return\n",
    "\n",
    "    setValue = {db_txTableColumns[0]: 1}\n",
    "    whereCondition = {db_txTableColumns[4]: matchList}\n",
    "    updateTable(db_txTableName, setValue, whereCondition)\n",
    "\n",
    "    print('txData updated')\n",
    "\n",
    "\n",
    "def writeTx(txRawData):\n",
    "    print('\\tFunction \"writeTx\" executed')\n",
    "    txDataList = []\n",
    "    for txDatum in txRawData:\n",
    "        txList = [txDatum.commitYN, txDatum.sender, txDatum.amount, txDatum.receiver, txDatum.uuid, txDatum.txValidity]\n",
    "        txDataList.append(txList)\n",
    "\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "\n",
    "    txData = selectTable(db_txTableName, db_txTableColumns, engine)\n",
    "    newTxData = pd.DataFrame(txDataList, columns = db_txTableColumns)\n",
    "    mergedTxData = pd.concat([txData, newTxData], axis=0).reset_index(drop=True)\n",
    "\n",
    "    replaceTable(db_txTableName, db_txTableColumns)\n",
    "    try:\n",
    "        to_varchar = {c: types.VARCHAR(mergedTxData[c].str.len().max()) for c in\n",
    "                      mergedTxData.columns[mergedTxData.dtypes == 'object'].tolist()}\n",
    "        mergedTxData.to_sql(db_txTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "    except:\n",
    "        print('Data save error, It seems to have an integrity or type problem.')\n",
    "        to_varchar = {c: types.VARCHAR(txData[c].str.len().max()) for c in\n",
    "                      txData.columns[txData.dtypes == 'object'].tolist()}\n",
    "        txData.to_sql(db_txTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "        return 0\n",
    "\n",
    "    print('txData written to txData.csv.')\n",
    "    return 1\n",
    "\n",
    "\n",
    "def readTx(tableName, columns):  # 거래내역 읽기 채굴할때 호출 블록에 없는 데이터들 불러옴\n",
    "    print('\\tFunction \"readTx\" executed')\n",
    "\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "\n",
    "    importedTx = []\n",
    "\n",
    "    txReader = selectTable(tableName, columns, engine)\n",
    "    for i in range(len(txReader)):\n",
    "        row = txReader.loc[i]\n",
    "        if row[0] == '0':  # find unmined txData\n",
    "            line = txData(row[0], row[1], row[2], row[3], row[4], row[5])\n",
    "            importedTx.append(line)\n",
    "    print(\"Pulling txData from csv...\")\n",
    "    return importedTx\n",
    "\n",
    "def getTxData():\n",
    "    print('\\tFunction \"getTxData\" executed')\n",
    "    strTxData = ''\n",
    "    importedTx = readTx(db_txTableName, db_txTableColumns)\n",
    "    if len(importedTx) > 0:\n",
    "        for i in importedTx:\n",
    "            transaction = \"[\"+ i.uuid + \"]\" \"UserKey \" + i.sender + \" sent \" + i.amount + \" bitTokens to UserKey \" + i.receiver + \". \"\n",
    "            print(transaction)\n",
    "            strTxData += transaction\n",
    "    return strTxData\n",
    "\n",
    "\n",
    "def mineNewBlock(difficulty=g_difficulty, tableName=db_blockTableName, columns=db_blockTableColumns):\n",
    "    print('\\tFunction \"mineNewBlock\" executed')\n",
    "    blockchain = readBlockchain(tableName, columns)\n",
    "    strTxData = getTxData()\n",
    "    if strTxData == '':\n",
    "        print('txdata not found, so mining aborted')\n",
    "        return\n",
    "    timestamp = time.time()\n",
    "    proof = 0\n",
    "    newBlcokFound = False\n",
    "\n",
    "    print(\"Mining  blocks\")\n",
    "\n",
    "    while not newBlcokFound:\n",
    "        newBlockAttempt = generateNextBlock(blockchain, strTxData, timestamp, proof)\n",
    "        if newBlockAttempt.currentHash[\n",
    "           0:difficulty] == '0' * difficulty:  # 0부터 설정 난이도까지 0*4 = 0000이냐 로 묻는 것 - 즉 난이도를 만족하냐?\n",
    "            stopTime = time.time()\n",
    "            timer = stopTime - timestamp\n",
    "            print('New block found with proof', proof, 'in', round(timer, 2), 'seconds.')\n",
    "            newBlockFound = True\n",
    "        else:\n",
    "            proof += 1\n",
    "\n",
    "    blockchain.append(newBlockAttempt)\n",
    "    writeBlockchain(blockchain)\n",
    "\n",
    "def mine():\n",
    "    print('\\tFunction \"mine\" executed')\n",
    "    mineNewBlock()\n",
    "\n",
    "def isSameBlock(block1, block2):\n",
    "    print('\\tFunction \"isSameBlock\" executed')\n",
    "    if str(block1.index) != str(block2.index):\n",
    "        return False\n",
    "    elif str(block1.previousHash) != str(block2.previousHash):\n",
    "        return False\n",
    "    elif str(block1.timestamp) != str(block2.timestamp):\n",
    "        return False\n",
    "    elif str(block1.data) != str(block2.data):\n",
    "        return False\n",
    "    elif str(block1.currentHash) != str(block2.currentHash):\n",
    "        return False\n",
    "    elif str(block1.proof) != str(block2.proof):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def isValidNewBlock(newBlock, previousBlock):\n",
    "    print('\\tFunction \"isValidNewBlock\" executed')\n",
    "    if int(previousBlock.index) + 1 != int(newBlock.index):\n",
    "        print('Indices Do Not Match Up')\n",
    "        return False\n",
    "    elif previousBlock.currentHash != newBlock.previousHash:\n",
    "        print(\"Previous hash does not match\")\n",
    "        return False\n",
    "    elif calculateHashForBlock(newBlock) != newBlock.currentHash:\n",
    "        print(\"Hash is invalid\")\n",
    "        return False\n",
    "    elif newBlock.currentHash[0:g_difficulty] != '0' * g_difficulty:\n",
    "        print(\"Hash difficulty is invalid\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def newtx(txToMining, validity):\n",
    "    print('\\tFunction \"newtx\" executed')\n",
    "    newtxData = []\n",
    "    # transform given data to txData object\n",
    "    for line in txToMining:\n",
    "        tx = txData(0, line['sender'], line['amount'], line['receiver'], uuid.uuid4(), validity)\n",
    "        newtxData.append(tx)\n",
    "\n",
    "    # limitation check : max 5 tx\n",
    "    if len(newtxData) > 5:\n",
    "        print('number of requested tx exceeds limitation')\n",
    "        return -1\n",
    "\n",
    "    if writeTx(newtxData) == 0:\n",
    "        print(\"file write error on txData\")\n",
    "        return -2\n",
    "    return 1\n",
    "\n",
    "def isValidChain(bcToValidate):\n",
    "    print('\\tFunction \"isValidChain\" executed')\n",
    "    genesisBlock = []\n",
    "    bcToValidateForBlock = []\n",
    "\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    blockReader = selectTable(db_blockTableName, db_blockTableColumns, engine)\n",
    "    for i in range(len(blockReader)):\n",
    "        line = blockReader.loc[i]\n",
    "        block = Block(line[0], line[1], line[2], line[3], line[4], line[5])\n",
    "        genesisBlock.append(block)\n",
    "\n",
    "    # transform given data to Block object\n",
    "    for line in bcToValidate:\n",
    "        block = Block(line['index'], line['previousHash'], line['timestamp'], line['data'], line['currentHash'], line['proof'])\n",
    "        bcToValidateForBlock.append(block)\n",
    "\n",
    "    #if it fails to read block data  from db(csv)\n",
    "    if not genesisBlock:\n",
    "        print(\"fail to read genesisBlock\")\n",
    "        return False\n",
    "\n",
    "    # compare the given data with genesisBlock\n",
    "    if not isSameBlock(bcToValidateForBlock[0], genesisBlock[0]):\n",
    "        print('Genesis Block Incorrect')\n",
    "        return False\n",
    "\n",
    "    for i in range(0, len(bcToValidateForBlock)):\n",
    "        if isSameBlock(genesisBlock[i], bcToValidateForBlock[i]) == False:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def addNode(queryStr):\n",
    "    print('\\tFunction \"addNode\" executed')\n",
    "    # save\n",
    "    txDataList = []\n",
    "    txDataList.append([queryStr[0], queryStr[1], 0])  # ip, port, # of connection fail\n",
    "\n",
    "    nodeData = selectTable(db_nodeListTableName, db_nodeListTableColumns, engine)\n",
    "    nodeDataList = []\n",
    "    for i in range(len(nodeData)):\n",
    "        row = nodeData.loc[i]\n",
    "        if row[0] == queryStr[0] and row[1] == queryStr[1]:\n",
    "            print(\"requested node is already exists\")\n",
    "            return -1\n",
    "        else:\n",
    "            nodeDataList.append(row)\n",
    "\n",
    "    if (len(nodeData) > 0):\n",
    "        nodeDataFrame = pd.DataFrame(nodeDataList, columns=db_nodeListTableColumns)\n",
    "    else:\n",
    "        # this is 1st time of creating node list\n",
    "        nodeDataFrame = pd.DataFrame(txDataList, columns=db_nodeListTableColumns)\n",
    "\n",
    "    replaceTable(db_nodeListTableName, db_nodeListTableColumns)\n",
    "    try:\n",
    "        to_varchar = {c: types.VARCHAR(nodeDataFrame[c].str.len().max()) for c in\n",
    "                      nodeDataFrame.columns[nodeDataFrame.dtypes == 'object'].tolist()}\n",
    "        nodeDataFrame.to_sql(db_nodeListTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "    except:\n",
    "        print('Data save error, It seems to have an integrity or type problem.')\n",
    "        to_varchar = {c: types.VARCHAR(nodeData[c].str.len().max()) for c in\n",
    "                      nodeData.columns[nodeData.dtypes == 'object'].tolist()}\n",
    "        nodeData.to_sql(db_nodeListTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "        return 0\n",
    "\n",
    "    print('new node written to nodelist.csv.')\n",
    "    return 1\n",
    "\n",
    "\n",
    "def readNodes(tableName, columns):\n",
    "    print('\\tFunction \"readNodes\" executed')\n",
    "    importedNodes = []\n",
    "\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "\n",
    "    txReader = selectTable(tableName, columns, engine)\n",
    "    for i in range(len(txReader)):\n",
    "        row = txReader.loc[i]\n",
    "        line = [row[0], row[1]]\n",
    "        importedNodes.append(line)\n",
    "    print(\"Pulling txData from csv...\")\n",
    "    return importedNodes\n",
    "\n",
    "\n",
    "def broadcastNewBlock(blockchain):\n",
    "    print('\\tFunction \"broadcastNewBlock\" executed')\n",
    "    # newBlock  = getLatestBlock(blockchain) # get the latest block\n",
    "    importedNodes = readNodes(db_nodeListTableName, db_nodeListTableColumns)  # get server node ip and port\n",
    "    reqHeader = {'Content-Type': 'application/json; charset=utf-8'}\n",
    "    reqBody = []\n",
    "    nodeDataList = []\n",
    "\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "\n",
    "    for i in blockchain:\n",
    "        reqBody.append(i.__dict__)\n",
    "\n",
    "    if len(importedNodes) > 0:\n",
    "        for node in importedNodes:\n",
    "            try:\n",
    "                URL = \"http://\" + node[0] + \":\" + node[1] + g_receiveNewBlock  # http://ip:port/node/receiveNewBlock\n",
    "                res = requests.post(URL, headers=reqHeader, data=json.dumps(reqBody))\n",
    "                if res.status_code == 200:\n",
    "                    print(URL + \" sent ok.\")\n",
    "                    print(\"Response Message \" + res.text)\n",
    "                else:\n",
    "                    print(URL + \" responding error \" + res.status_code)\n",
    "            except:\n",
    "                print(URL + \" is not responding.\")\n",
    "                # write responding results\n",
    "                nodeData = selectTable(db_nodeListTableName, db_nodeListTableColumns, engine)\n",
    "                for i in range(len(nodeData)):\n",
    "                    row = nodeData.loc[i]\n",
    "                    if (row[0] == node[0] and row[1] == node[1]):\n",
    "                        print(\"connection failed \" + row[0] + \":\" + row[1] + \", number of fail \" + row[2])\n",
    "                        tmp = row[2]\n",
    "                        # too much fail, delete node\n",
    "                        if int(tmp) > g_maximumTry:\n",
    "                            print(row[0] + \":\" + row[\n",
    "                                1] + \" deleted from node list because of exceeding the request limit\")\n",
    "                        else:\n",
    "                            row[2] = int(tmp) + 1\n",
    "                            nodeDataList.append(row)\n",
    "                    else:\n",
    "                        nodeDataList.append(row)\n",
    "\n",
    "                if (len(nodeData) > 0):\n",
    "                    nodeDataFrame = pd.DataFrame(nodeDataList, columns=db_nodeListTableColumns)\n",
    "                    replaceTable(db_nodeListTableName, db_nodeListTableColumns)\n",
    "                    try:\n",
    "                        to_varchar = {c: types.VARCHAR(nodeDataFrame[c].str.len().max()) for c in\n",
    "                                      nodeDataFrame.columns[nodeDataFrame.dtypes == 'object'].tolist()}\n",
    "                        nodeDataFrame.to_sql(db_nodeListTableName, engine, if_exists='append', index=False,\n",
    "                                             dtype=to_varchar)\n",
    "                    except:\n",
    "                        print('Data save error, It seems to have an integrity or type problem.')\n",
    "                        to_varchar = {c: types.VARCHAR(nodeData[c].str.len().max()) for c in\n",
    "                                      nodeData.columns[nodeData.dtypes == 'object'].tolist()}\n",
    "                        nodeData.to_sql(db_nodeListTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "                else:\n",
    "                    print(\"caught exception while updating node list\")\n",
    "\n",
    "\n",
    "def compareMerge(bcDict):\n",
    "    print('\\tFunction \"compareMerge\" executed')\n",
    "    heldBlock = []\n",
    "    bcToValidateForBlock = []\n",
    "\n",
    "    # Read GenesisBlock\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    blockReader = selectTable(db_blockTableName, db_blockTableColumns, engine)\n",
    "    for i in range(len(blockReader)):\n",
    "        line = blockReader.loc[i]\n",
    "        block = Block(line[0], line[1], line[2], line[3], line[4], line[5])\n",
    "        heldBlock.append(block)\n",
    "\n",
    "    if len(blockReader) == 0:\n",
    "        print(\"file open error in compareMerge or No database exists\")\n",
    "        print(\"call initSvr if this server has just installed\")\n",
    "        return -1\n",
    "\n",
    "        # if it fails to read block data  from db(csv)\n",
    "    if len(heldBlock) == 0:\n",
    "        print(\"fail to read\")\n",
    "        return -2\n",
    "\n",
    "    # transform given data to Block object\n",
    "    for line in bcDict:\n",
    "        # print(type(line))\n",
    "        # index, previousHash, timestamp, data, currentHash, proof\n",
    "        block = Block(line['index'], line['previousHash'], line['timestamp'], line['data'], line['currentHash'],\n",
    "                      line['proof'])\n",
    "        bcToValidateForBlock.append(block)\n",
    "\n",
    "    # compare the given data with genesisBlock\n",
    "    if not isSameBlock(bcToValidateForBlock[0], heldBlock[0]):\n",
    "        print('Genesis Block Incorrect')\n",
    "        return -1\n",
    "\n",
    "    # check if broadcasted new block,1 ahead than > last held block\n",
    "\n",
    "    if isValidNewBlock(bcToValidateForBlock[-1], heldBlock[-1]) == False:\n",
    "\n",
    "        # latest block == broadcasted last block\n",
    "        if isSameBlock(heldBlock[-1], bcToValidateForBlock[-1]) == True:\n",
    "            print('latest block == broadcasted last block, already updated')\n",
    "            return 2\n",
    "        # select longest chain\n",
    "        elif len(bcToValidateForBlock) > len(heldBlock):\n",
    "            # validation\n",
    "            if isSameBlock(heldBlock[0], bcToValidateForBlock[0]) == False:\n",
    "                print(\"Block Information Incorrect #1\")\n",
    "                return -1\n",
    "            tempBlocks = [bcToValidateForBlock[0]]\n",
    "            for i in range(1, len(bcToValidateForBlock)):\n",
    "                if isValidNewBlock(bcToValidateForBlock[i], tempBlocks[i - 1]):\n",
    "                    tempBlocks.append(bcToValidateForBlock[i])\n",
    "                else:\n",
    "                    return -1\n",
    "            # [START] save it to csv\n",
    "            blockchainList = []\n",
    "            for block in bcToValidateForBlock:\n",
    "                blockList = [block.index, block.previousHash, str(block.timestamp), block.data,\n",
    "                             block.currentHash, block.proof]\n",
    "                blockchainList.append(blockList)\n",
    "\n",
    "            blockchainData = pd.DataFrame(blockchainList, columns=db_blockTableColumns)\n",
    "            blockWriter = pd.concat([blockReader, blockchainData], axis=0).reset_index(drop=True)\n",
    "            replaceTable(db_blockTableName, db_blockTableColumns)\n",
    "            try:\n",
    "                to_varchar = {c: types.VARCHAR(blockWriter[c].str.len().max()) for c in\n",
    "                              blockWriter.columns[blockWriter.dtypes == 'object'].tolist()}\n",
    "                blockWriter.to_sql(db_blockTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "            except:\n",
    "                print('Data save error, It seems to have an integrity or type problem.')\n",
    "                to_varchar = {c: types.VARCHAR(blockReader[c].str.len().max()) for c in\n",
    "                              blockReader.columns[blockReader.dtypes == 'object'].tolist()}\n",
    "                blockReader.to_sql(db_blockTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "            # [END] save it to csv\n",
    "            return 1\n",
    "        elif len(bcToValidateForBlock) < len(heldBlock):\n",
    "            # validation\n",
    "            # for i in range(0,len(bcToValidateForBlock)):\n",
    "            #    if isSameBlock(heldBlock[i], bcToValidateForBlock[i]) == False:\n",
    "            #        print(\"Block Information Incorrect #1\")\n",
    "            #        return -1\n",
    "            tempBlocks = [bcToValidateForBlock[0]]\n",
    "            for i in range(1, len(bcToValidateForBlock)):\n",
    "                if isValidNewBlock(bcToValidateForBlock[i], tempBlocks[i - 1]):\n",
    "                    tempBlocks.append(bcToValidateForBlock[i])\n",
    "                else:\n",
    "                    return -1\n",
    "            print(\"We have a longer chain\")\n",
    "            return 3\n",
    "        else:\n",
    "            print(\"Block Information Incorrect #2\")\n",
    "            return -1\n",
    "    else:  # very normal case (ex> we have index 100 and receive index 101 ...)\n",
    "        tempBlocks = [bcToValidateForBlock[0]]\n",
    "        for i in range(1, len(bcToValidateForBlock)):\n",
    "            if isValidNewBlock(bcToValidateForBlock[i], tempBlocks[i - 1]):\n",
    "                tempBlocks.append(bcToValidateForBlock[i])\n",
    "            else:\n",
    "                print(\"Block Information Incorrect #2 \" + tempBlocks.__dict__)\n",
    "                return -1\n",
    "\n",
    "        print(\"new block good\")\n",
    "\n",
    "        # validation\n",
    "        for i in range(0, len(heldBlock)):\n",
    "            if isSameBlock(heldBlock[i], bcToValidateForBlock[i]) == False:\n",
    "                print(\"Block Information Incorrect #1\")\n",
    "                return -1\n",
    "        # [START] save it to csv\n",
    "        blockchainList = []\n",
    "        for block in bcToValidateForBlock:\n",
    "            blockList = [block.index, block.previousHash, str(block.timestamp), block.data, block.currentHash,\n",
    "                         block.proof]\n",
    "            blockchainList.append(blockList)\n",
    "        blockchainData = pd.DataFrame(blockchainList, columns=db_blockTableColumns)\n",
    "        blockWriter = pd.concat([blockReader, blockchainData], axis=0).reset_index(drop=True)\n",
    "        replaceTable(db_blockTableName, db_blockTableColumns)\n",
    "        try:\n",
    "            to_varchar = {c: types.VARCHAR(blockWriter[c].str.len().max()) for c in\n",
    "                          blockWriter.columns[blockWriter.dtypes == 'object'].tolist()}\n",
    "            blockWriter.to_sql(db_blockTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "        except:\n",
    "            print('Data save error, It seems to have an integrity or type problem.')\n",
    "            to_varchar = {c: types.VARCHAR(blockReader[c].str.len().max()) for c in\n",
    "                          blockReader.columns[blockReader.dtypes == 'object'].tolist()}\n",
    "            blockReader.to_sql(db_blockTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "        # [END] save it to csv\n",
    "        return 1\n",
    "\n",
    "\n",
    "def initSvr():\n",
    "    print('\\tFunction \"initSvr\" executed')\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    last_line_number = len(selectTable(db_nodeListTableName, db_nodeListTableColumns, engine))\n",
    "    # 1. check if we have a node list file\n",
    "    # if we don't have, let's request node list\n",
    "    if last_line_number == 0:\n",
    "        # get nodes...\n",
    "        for key, value in g_nodeList.items():\n",
    "            URL = 'http://' + key + ':' + value + '/node/getNode'\n",
    "            try:\n",
    "                res = requests.get(URL)\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                continue\n",
    "            if res.status_code == 200:\n",
    "                print(res.text)\n",
    "                tmpNodeLists = json.loads(res.text)\n",
    "                for node in tmpNodeLists:\n",
    "                    addNode(node)\n",
    "\n",
    "    # 2. check if we have a blockchain data file\n",
    "    last_line_number = len(selectTable(db_blockTableName, db_blockTableColumns, engine))\n",
    "    blockchainList = []\n",
    "    if last_line_number == 0:\n",
    "        # get Block Data...\n",
    "        for key, value in g_nodeList.items():\n",
    "            URL = 'http://' + key + ':' + value + '/block/getBlockData'\n",
    "            try:\n",
    "                res = requests.get(URL)\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                continue\n",
    "            if res.status_code == 200:\n",
    "                print(res.text)\n",
    "                tmpbcData = json.loads(res.text)\n",
    "                for line in tmpbcData:\n",
    "                    # print(type(line))\n",
    "                    # index, previousHash, timestamp, data, currentHash, proof\n",
    "                    block = [line['index'], line['previousHash'], line['timestamp'], line['data'], line['currentHash'],\n",
    "                             line['proof']]\n",
    "                    blockchainList.append(block)\n",
    "\n",
    "                blockchainData = pd.DataFrame(blockchainList, columns=db_blockTableColumns)\n",
    "                replaceTable(db_blockTableName, db_blockTableColumns)\n",
    "                try:\n",
    "                    to_varchar = {c: types.VARCHAR(blockchainData[c].str.len().max()) for c in\n",
    "                                  blockchainData.columns[blockchainData.dtypes == 'object'].tolist()}\n",
    "                    blockchainData.to_sql(db_blockTableName, engine, if_exists='append', index=False, dtype=to_varchar)\n",
    "                except Exception as e:\n",
    "                    print('Data save error in initSvr()', e)\n",
    "    return 1\n",
    "\n",
    "class myHandler(BaseHTTPRequestHandler):\n",
    "\n",
    "    #def __init__(self, request, client_address, server):\n",
    "    #    BaseHTTPRequestHandler.__init__(self, request, client_address, server)\n",
    "\n",
    "    # Handler for the GET requests\n",
    "    def do_GET(self):\n",
    "        data = []  # response json data\n",
    "        if None != re.search('/block/*', self.path):\n",
    "            self.send_response(200)\n",
    "            self.send_header('Content-type', 'application/json')\n",
    "            self.end_headers()\n",
    "\n",
    "            # 약점 : 사이즈가 커서 한번에 주면 서버가 죽을 수 있다. 나눠서 줘야함( 페이징 처리 / 게시물의 범위 )\n",
    "            if None != re.search('/block/getBlockData', self.path):\n",
    "                # TODO: range return (~/block/getBlockData?from=1&to=300)\n",
    "                # queryString = urlparse(self.path).query.split('&')\n",
    "\n",
    "                block = readBlockchain(db_blockTableName, db_blockTableColumns, mode = 'external')\n",
    "\n",
    "                if block == None :\n",
    "                    print(\"No Block Exists\")\n",
    "                    data.append(\"no data exists\")\n",
    "                else :\n",
    "                    for i in block:\n",
    "                        print(i.__dict__)\n",
    "                        data.append(i.__dict__)\n",
    "\n",
    "                self.wfile.write(bytes(json.dumps(data, sort_keys=True, indent=4), \"utf-8\"))\n",
    "\n",
    "            elif None != re.search('/block/generateBlock', self.path):\n",
    "                t = threading.Thread(target=mine)\n",
    "                t.start()\n",
    "                data.append(\"{mining is underway:check later by calling /block/getBlockData}\")\n",
    "                self.wfile.write(bytes(json.dumps(data, sort_keys=True, indent=4), \"utf-8\"))\n",
    "            else:\n",
    "                data.append(\"{info:no such api}\")\n",
    "                self.wfile.write(bytes(json.dumps(data, sort_keys=True, indent=4), \"utf-8\"))\n",
    "\n",
    "        elif None != re.search('/node/*', self.path):\n",
    "            self.send_response(200)\n",
    "            self.send_header('Content-type', 'application/json')\n",
    "            self.end_headers()\n",
    "            if None != re.search('/node/addNode', self.path):\n",
    "                queryStr = urlparse(self.path).query.split(':')\n",
    "                print(\"client ip : \"+self.client_address[0]+\" query ip : \"+queryStr[0])\n",
    "                if self.client_address[0] != queryStr[0]:\n",
    "                    data.append(\"your ip address doesn't match with the requested parameter\")\n",
    "                else:\n",
    "                    res = addNode(queryStr)\n",
    "                    if res == 1:\n",
    "                        importedNodes = readNodes(db_nodeListTableName, db_nodeListTableColumns)\n",
    "                        data = importedNodes\n",
    "                        print(\"node added okay\")\n",
    "                    elif res == 0 :\n",
    "                        data.append(\"caught exception while saving\")\n",
    "                    elif res == -1 :\n",
    "                        importedNodes = readNodes(db_nodeListTableName, db_nodeListTableColumns)\n",
    "                        data = importedNodes\n",
    "                        data.append(\"requested node is already exists\")\n",
    "                self.wfile.write(bytes(json.dumps(data, sort_keys=True, indent=4), \"utf-8\"))\n",
    "            elif None != re.search('/node/getNode', self.path):\n",
    "                importedNodes = readNodes(db_nodeListTableName, db_nodeListTableColumns)\n",
    "                data = importedNodes\n",
    "                self.wfile.write(bytes(json.dumps(data, sort_keys=True, indent=4), \"utf-8\"))\n",
    "        else:\n",
    "            self.send_response(403)\n",
    "            self.send_header('Content-Type', 'application/json')\n",
    "            self.end_headers()\n",
    "        # ref : https://mafayyaz.wordpress.com/2013/02/08/writing-simple-http-server-in-python-with-rest-and-json/\n",
    "\n",
    "    def do_POST(self):\n",
    "        print(\"POSTPOSTPOSTPOSTPOST\")\n",
    "        if None != re.search('/block/*', self.path):\n",
    "            self.send_response(200)\n",
    "            self.send_header('Content-type', 'application/json')\n",
    "            self.end_headers()\n",
    "\n",
    "            if None != re.search('/block/validateBlock/*', self.path):\n",
    "                ctype, pdict = cgi.parse_header(self.headers['content-type'])\n",
    "                #print(ctype) #print(pdict)\n",
    "\n",
    "                if ctype == 'application/json':\n",
    "                    content_length = int(self.headers['Content-Length'])\n",
    "                    post_data = self.rfile.read(content_length)\n",
    "                    receivedData = post_data.decode('utf-8')\n",
    "                    print(type(receivedData))\n",
    "                    tempDict = json.loads(receivedData)  # load your str into a list #print(type(tempDict))\n",
    "                    if isValidChain(tempDict) == True :\n",
    "                        tempDict.append(\"validationResult:normal\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "                    else :\n",
    "                        tempDict.append(\"validationResult:abnormal\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "            elif None != re.search('/block/newtx', self.path):\n",
    "                ctype, pdict = cgi.parse_header(self.headers['content-type'])\n",
    "                if ctype == 'application/json':\n",
    "                    content_length = int(self.headers['Content-Length'])\n",
    "                    post_data = self.rfile.read(content_length)\n",
    "                    receivedData = post_data.decode('utf-8')\n",
    "                    print(type(receivedData))\n",
    "                    tempDict = json.loads(receivedData)\n",
    "                    res = newtx(tempDict)\n",
    "                    if  res == 1 :\n",
    "                        tempDict.append(\"accepted : it will be mined later\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "                    elif res == -1 :\n",
    "                        tempDict.append(\"declined : number of request txData exceeds limitation\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "                    elif res == -2 :\n",
    "                        tempDict.append(\"declined : error on data read or write\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "                    else :\n",
    "                        tempDict.append(\"error : requested data is abnormal\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "\n",
    "        elif None != re.search('/node/*', self.path):\n",
    "            self.send_response(200)\n",
    "            self.send_header('Content-type', 'application/json')\n",
    "            self.end_headers()\n",
    "            if None != re.search(g_receiveNewBlock, self.path): # /node/receiveNewBlock\n",
    "                content_length = int(self.headers['Content-Length'])\n",
    "                post_data = self.rfile.read(content_length)\n",
    "                receivedData = post_data.decode('utf-8')\n",
    "                tempDict = json.loads(receivedData)  # load your str into a list\n",
    "                print(tempDict)\n",
    "                res = compareMerge(tempDict)\n",
    "                if res == -1: # internal error\n",
    "                    tempDict.append(\"internal server error\")\n",
    "                elif res == -2 : # block chain info incorrect\n",
    "                    tempDict.append(\"block chain info incorrect\")\n",
    "                elif res == 1: #normal\n",
    "                    tempDict.append(\"accepted\")\n",
    "                elif res == 2: # identical\n",
    "                    tempDict.append(\"already updated\")\n",
    "                elif res == 3: # we have a longer chain\n",
    "                    tempDict.append(\"we have a longer chain\")\n",
    "                self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "        else:\n",
    "            self.send_response(404)\n",
    "            self.send_header('Content-Type', 'application/json')\n",
    "            self.end_headers()\n",
    "\n",
    "        return\n",
    "\n",
    "class ThreadedHTTPServer(ThreadingMixIn, HTTPServer):\n",
    "    \"\"\"Handle requests in a separate thread.\"\"\"\n",
    "\n",
    "try:\n",
    "\n",
    "    # Create a web server and define the handler to manage the\n",
    "    # incoming request\n",
    "    # server = HTTPServer(('', PORT_NUMBER), myHandler)\n",
    "    server = ThreadedHTTPServer(('', PORT_NUMBER), myHandler)\n",
    "    print('Started httpserver on port ', PORT_NUMBER)\n",
    "\n",
    "    initSvr()\n",
    "    # Wait forever for incoming http requests\n",
    "    server.serve_forever()\n",
    "\n",
    "except (KeyboardInterrupt, Exception) as e:\n",
    "    print('^C received, shutting down the web server')\n",
    "    print(e)\n",
    "    server.socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFunction \"createTable\" executed\n",
      "123123\n"
     ]
    }
   ],
   "source": [
    "createTable(db_txTableName, db_txTableColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeUpdateQuery(tableName, setValue, whereCondition):\n",
    "    setValueInput = ''\n",
    "    for key, value in setValue.items():\n",
    "        setValueInput += str(key)\n",
    "        setValueInput += ' = '\n",
    "        setValueInput += str(value)\n",
    "        setValueInput += ', '\n",
    "    setValueInput = setValueInput.rstrip(', ')\n",
    "        \n",
    "    whereConditionInput = ''\n",
    "    for key, value in setValue.items():\n",
    "        if type(value) == list:\n",
    "            for eachValue in value:\n",
    "                setValueInput += str(key)\n",
    "                setValueInput += ' = '\n",
    "                setValueInput += str(eachValue)\n",
    "                setValueInput += ' | '\n",
    "            setValueInput = setValueInput.rstrip(' | ')\n",
    "            setValueInput += ' & '\n",
    "        else:\n",
    "            whereConditionInput += str(key)\n",
    "            whereConditionInput += ' = '\n",
    "            whereConditionInput += str(value)\n",
    "            whereConditionInput += ' & '\n",
    "    whereConditionInput = whereConditionInput.rstrip(' & ')\n",
    "    \n",
    "    updateQuery = 'UPDATE %s SET %s WHERE %s' % (tableName, setValueInput, whereConditionInput)\n",
    "    return updateQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTable(tableName, setValue, whereCondition):\n",
    "    print('\\tFunction \"updateTable\" executed')\n",
    "    \n",
    "    connectComplete = False\n",
    "    cursorComplete = False\n",
    "    \n",
    "    try:\n",
    "        connectInfo = '%s/%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "        oracleConnect = oci.connect(connectInfo)\n",
    "        connectComplete = True\n",
    "        oracleCursor = oracleConnect.cursor()\n",
    "        cursorComplete = True\n",
    "        \n",
    "        updateQuery = makeUpdateQuery(tableName, setValue, whereCondition)\n",
    "        oracleCursor.execute(updateQuery)\n",
    "        oracleConnect.commit()\n",
    "        oracleCursor.close()\n",
    "        oracleConnect.close\n",
    "        \n",
    "    except:\n",
    "        if (cursorComplete == True):\n",
    "            oracleCursor.close()\n",
    "        if (connectComplete == True):\n",
    "            oracleConnect.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moneyTransferSearch(sender, senderUserkey, receiver, receiverUserkey, amount, uuid): #이름 키 이름 키 금액 uuid\n",
    "    try:\n",
    "        cursor.execute(\"SELECT balance FROM BPS_USERS where username = '{}' and userkey = '{}'\".format(sender, senderUserkey))  # 송신자, 송신자의 유져 키를 받아서 이걸로 잔고조회를 한다.\n",
    "        userrow = cursor.fetchall()\n",
    "    except:\n",
    "        print(\"송신자 정보 입력 오류\")\n",
    "\n",
    "    try:\n",
    "        cursor.execute(\"SELECT txValidity FROM BPS_TXDATA where uuid = '{}'\".format(uuid))  # 개별 트랜잭션번호로 트랜잭션 유효성을 조회\n",
    "        txrow = cursor.fetchall()\n",
    "    except:\n",
    "        print(\"트랜잭션 정보 조회 오류\")\n",
    "\n",
    "    moneyTransferCommit(sender, senderUserkey, receiver, receiverUserkey, amount, uuid, userrow, txrow)\n",
    "\n",
    "def moneyTransferCommit(sender, senderUserkey, receiver, receiverUserkey, amount, uuid, userrow, txrow):\n",
    "    if userrow[0][0] >= amount and txrow[0][0] == None: # 보내려는 금액보다 잔고가 크고 개별 트랙잭션 유효성이 null 값일 때 아래 구문 실행.\n",
    "        cursor.execute(\"UPDATE BPS_USERS SET BALANCE = BALANCE-{} WHERE USERNAME = '{}' and userkey = '{}'\".format(amount, sender, senderUserkey)) # 송신자 계좌에서 빼준다.\n",
    "        cursor.execute(\"UPDATE BPS_USERS SET BALANCE = BALANCE+{} WHERE USERNAME = '{}' and userkey = '{}'\".format(amount, receiver, receiverUserkey)) # 수신자 계좌에서 더해준다.\n",
    "        cursor.execute(\"UPDATE BPS_TxData SET txValidity = 1 WHERE uuid = '{}'\".format(uuid)) # 기본은 null, 1은 유효한 거래\n",
    "    elif userrow[0][0] < amount:\n",
    "        cursor.execute(\"UPDATE BPS_TxData SET txValidity = 0 WHERE uuid = '{}'\".format(uuid)) # 기본은 null, 0은 유효하지 않은 거래\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "cursor.close() # cursor 객체 닫기\n",
    "\n",
    "# Oracle 서버와 연결 끊기\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "PORT_NUMBER = 8099\n",
    "db_ip = '192.168.110.3'\n",
    "db_port = '1522'\n",
    "db_serviceName = 'xe'\n",
    "db_id = 'DJ2019'\n",
    "db_pw = 'DJ2019'\n",
    "db_userTableName = 'BPS_USERS'\n",
    "db_userTableColumns = ('NAME', 'KEY', 'BALANCE')\n",
    "db_blockTableName = 'BPS_BLOCK'\n",
    "db_blockTableColumns = ('BLOCKINDEX', 'PREVIOUSHASH', 'TIMESTAMP', 'DATA', 'CURRENTHASH', 'PROOF')\n",
    "db_txTableName = 'BPS_TXDATA'\n",
    "db_txTableColumns = ('COMMIT_YN', 'SENDER', 'AMOUNT', 'RECEIVER', 'UUID', 'TX_VALIDITY')\n",
    "db_nodeListTableName = 'BPS_NODELIST'\n",
    "db_nodeListTableColumns = ('IP', 'PORT', 'CONNECTION_FAIL')\n",
    "g_receiveNewBlock = \"/node/receiveNewBlock\"\n",
    "g_difficulty = 4\n",
    "g_maximumTry = 100\n",
    "g_nodeList = {'trustedServerAddress':'8099'}\n",
    "\n",
    "connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "engine = create_engine(connectInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, types\n",
    "import cx_Oracle as oci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block:\n",
    "\n",
    "    def __init__(self, index, previousHash, timestamp, data, currentHash, proof):\n",
    "        self.index = index\n",
    "        self.previousHash = previousHash\n",
    "        self.timestamp = timestamp\n",
    "        self.data = data\n",
    "        self.currentHash = currentHash\n",
    "        self.proof = proof\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n",
    "\n",
    "class txData:\n",
    "\n",
    "    def __init__(self, commitYN, sender, amount, receiver, uuid):\n",
    "        self.commitYN = commitYN\n",
    "        self.sender = sender\n",
    "        self.amount = amount\n",
    "        self.receiver = receiver\n",
    "        self.uuid = uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCreateTableQuery(tableName, columns):\n",
    "    print('\\tFunction \"makeCreateTableQuery\" executed')\n",
    "    \n",
    "    if (tableName == 'BPS_BLOCK'):\n",
    "        createTableQuery = \"CREATE TABLE BPS_BLOCK(\\\n",
    "        %s NUMBER NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s VARCHAR2 (100) NOT NULL, \\\n",
    "        %s NUMBER NOT NULL, \\\n",
    "        CONSTRAINTS PK_BPS_BLOCK PRIMARY KEY(BLOCKINDEX) \\\n",
    "        )\" % columns\n",
    "    \n",
    "    return createTableQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDropTableQuery(tableName):\n",
    "    print('\\tFunction \"makeDropTableQuery\" executed')\n",
    "\n",
    "    dropTableQuery = 'DROP TABLE %s' % tableName\n",
    "    \n",
    "    return dropTableQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectTable(tableName, columns, engine):\n",
    "    print('\\tFunction \"selectTable\" executed')\n",
    "    selectQuery = 'SELECT * FROM %s' % tableName\n",
    "    try:\n",
    "        resultData = pd.read_sql_query(selectQuery, engine)\n",
    "    except:\n",
    "        print('Table select error, There are no table named \"%s\" in db. \\n It will be created' % tableName)\n",
    "        createTable(tableName, columns)\n",
    "        resultData = pd.read_sql_query(selectQuery, engine)\n",
    "    return resultData\n",
    "\n",
    "# def selectTableWhere(tableName, columns, engine, whereCondition):\n",
    "#     print('\\tFunction \"selectTable\" executed')\n",
    "    \n",
    "#     whereConditionInput = ''\n",
    "#     for key, value in setValue.items():\n",
    "#         whereConditionInput += str(key)\n",
    "#         whereConditionInput += ' = '\n",
    "#         whereConditionInput += str(value)\n",
    "#         whereConditionInput += ', '\n",
    "#     whereConditionInput = whereConditionInput.rstrip(', ')\n",
    "    \n",
    "#     selectQuery = 'SELECT * FROM %s WHERE %s' % (tableName, whereConditionInput)\n",
    "#     try:\n",
    "#         resultData = pd.read_sql_query(selectQuery, engine)\n",
    "#     except:\n",
    "#         print('Table select error, There are no table named \"%s\" in db. \\n It will be created' % tableName)\n",
    "#         createTable(tableName, columns)\n",
    "#         resultData = pd.read_sql_query(selectQuery, engine)\n",
    "#     return resultData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTable(tableName, columns):\n",
    "    print('\\tFunction \"createTable\" executed')\n",
    "    \n",
    "    connectComplete = False\n",
    "    cursorComplete = False\n",
    "    \n",
    "    try:\n",
    "        connectInfo = db_id + '/' + db_pw + '@' + db_ip + ':' + db_port + '/' + db_serviceName\n",
    "        oracleConnect = oci.connect(connectInfo)\n",
    "        connectComplete = True\n",
    "        oracleCursor = oracleConnect.cursor()\n",
    "        cursorComplete = True\n",
    "        \n",
    "        createTableQuery = makeCreateTableQuery(tableName, columns)\n",
    "        oracleCursor.execute(createTableQuery)\n",
    "        oracleConnect.commit()\n",
    "        oracleCursor.close()\n",
    "        oracleConnect.close\n",
    "        \n",
    "    except:\n",
    "        if (cursorComplete == True):\n",
    "            oracleCursor.close()\n",
    "        if (connectComplete == True):\n",
    "            oracleConnect.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceTable(tableName, columns):\n",
    "    print('\\tFunction \"replaceTable\" executed')\n",
    "    connectComplete = False\n",
    "    cursorComplete = False\n",
    "    \n",
    "    try:\n",
    "        connectInfo = db_id + '/' + db_pw + '@' + db_ip + ':' + db_port + '/' + db_serviceName\n",
    "        oracleConnect = oci.connect(connectInfo)\n",
    "        connectComplete = True\n",
    "        oracleCursor = oracleConnect.cursor()\n",
    "        cursorComplete = True\n",
    "        \n",
    "        dropTableQuery = makeDropTableQuery(tableName)\n",
    "        oracleCursor.execute(dropTableQuery)\n",
    "        createTableQuery = makeCreateTableQuery(tableName, columns)\n",
    "        oracleCursor.execute(createTableQuery)\n",
    "        oracleConnect.commit()\n",
    "        oracleCursor.close()\n",
    "        oracleConnect.close\n",
    "        \n",
    "    except:\n",
    "        if (cursorComplete == True):\n",
    "            oracleCursor.close()\n",
    "        if (connectComplete == True):\n",
    "            oracleConnect.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeBlockchain(blockchain):\n",
    "    \n",
    "    print('\\tFunction \"writeBlockchain\" executed')\n",
    "    blockchainList = []\n",
    "    for block in blockchain:\n",
    "        blockList = [block.index, str(block.previousHash), str(block.timestamp), str(block.data), str(block.currentHash), block.proof]\n",
    "        blockchainList.append(blockList)\n",
    "    \n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    \n",
    "    blockReader = selectTable(db_blockTableName, db_blockTableColumns, engine)\n",
    "\n",
    "    lastLineNumber = len(blockReader)\n",
    "    for i in range(lastLineNumber):\n",
    "        lineNumber = i + 1\n",
    "        if (lineNumber == lastLineNumber):\n",
    "            line = blockReader.loc[i]\n",
    "            lastBlock = Block(line[0], line[1], line[2], line[3], line[4], line[5])\n",
    "    \n",
    "    try:\n",
    "        if (lastBlock.index + 1 != int(blockchainList[-1][0]) or lastLineNumber + 1 != len(blockchainList)):\n",
    "            print(\"Index sequence mismatch\")\n",
    "            if (lastBlock.index == int(blockchainList[-1][0])):\n",
    "                print(\"DB has already been updated\")\n",
    "            return\n",
    "        \n",
    "    except:\n",
    "        print('Index search error, There are no data or Existing table have problems. \\n It will be replaced by full data.')\n",
    "        pass\n",
    "    \n",
    "    blockWriter = pd.DataFrame(blockchainList, columns = db_blockTableColumns)\n",
    "\n",
    "    # convert type to varchar if the types of the columns of a dataframe is object\n",
    "    replaceTable(db_blockTableName, db_blockTableColumns)\n",
    "    try:\n",
    "        to_varchar = {c:types.VARCHAR(blockWriter[c].str.len().max()) for c in blockWriter.columns[blockWriter.dtypes == 'object'].tolist()}\n",
    "        blockWriter.to_sql(db_blockTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "        print('Blockchain written to db')\n",
    "    except:\n",
    "        print('Data save error, It seems to have an integrity or type problem.')\n",
    "        to_varchar = {c:types.VARCHAR(blockReader[c].str.len().max()) for c in blockReader.columns[blockReader.dtypes == 'object'].tolist()}\n",
    "        blockReader.to_sql(db_blockTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "    \n",
    "    # update txData cause it has been mined.\n",
    "    for block in blockchain:\n",
    "        updateTx(block)\n",
    "\n",
    "    print('Broadcasting new block to other nodes')\n",
    "    broadcastNewBlock(blockchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readBlockchain(tableName = db_blockTableName, columns = db_blockTableColumns, mode = 'internal'):\n",
    "    \n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    \n",
    "    print(\"readBlockChain\")\n",
    "    importedBlockChain = []\n",
    "    \n",
    "    blockReader = selectTable(tableName, columns, engine)\n",
    "    try:\n",
    "        for i in range(len(blockReader)):\n",
    "            line = blockReader.loc[i]\n",
    "            block = Block(line[0],line[1],line[2],line[3],str(line[4]),str(line[5]) )\n",
    "            importedBlockChain.append(block)\n",
    "        print(\"success pulling blockchain from DB\")\n",
    "        return importedBlockChain\n",
    "    except:\n",
    "        if mode == 'internal':\n",
    "            return writeBlockchain(blockchain)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTx(blockData) :\n",
    "\n",
    "    phrase = re.compile(r\"\\w+[-]\\w+[-]\\w+[-]\\w+[-]\\w+\") # [6b3b3c1e-858d-4e3b-b012-8faac98b49a8]UserID hwang sent 333 bitTokens to UserID kim.\n",
    "    matchList = phrase.findall(blockData.data)\n",
    "\n",
    "    if len(matchList) == 0 :\n",
    "        print (\"No Match Found! \" + str(blockData.data) + \"block idx: \" + str(blockData.index))\n",
    "        return\n",
    "\n",
    "    setValue = {db_txTableColumns[0]:1}\n",
    "    whereCondition = {db_txTableColumns[4]:matchList}\n",
    "    updateTable(db_txTableName, setValue, whereCondition)\n",
    "    \n",
    "    print('txData updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTx(txRawData):\n",
    "    txDataList = []\n",
    "    for txDatum in txRawData:\n",
    "        txList = [txDatum.commitYN, txDatum.sender, txDatum.amount, txDatum.receiver, txDatum.uuid, txDatum.txValidity]\n",
    "        txDataList.append(txList)\n",
    "        \n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "\n",
    "    txData = selectTable(db_TxTableName, db_TxTableColumns, engine)\n",
    "    newTxData = pd.DataFrame(txDataList, columns=db_TxTableColumns)\n",
    "    mergedTxData = pd.concat([txData, newTxData], axis = 0).reset_index(drop = True)\n",
    "\n",
    "    replaceTable(db_TxTableName, db_TxTableColumns)\n",
    "    try:\n",
    "        to_varchar = {c:types.VARCHAR(mergedTxData[c].str.len().max()) for c in mergedTxData.columns[mergedTxData.dtypes == 'object'].tolist()}\n",
    "        mergedTxData.to_sql(db_TxTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "    except:\n",
    "        print('Data save error, It seems to have an integrity or type problem.')\n",
    "        to_varchar = {c:types.VARCHAR(txData[c].str.len().max()) for c in txData.columns[txData.dtypes == 'object'].tolist()}\n",
    "        txData.to_sql(db_TxTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "        return 0\n",
    "            \n",
    "    print('txData written to txData.csv.')\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTx(tableName, columns): #거래내역 읽기 채굴할때 호출 블록에 없는 데이터들 불러옴\n",
    "    print(\"readTx\")\n",
    "    \n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    \n",
    "    importedTx = []\n",
    "\n",
    "    txReader = selectTable(tableName, columns, engine)\n",
    "    for i in range(len(txReader)):\n",
    "        row = txReader.loc[i]\n",
    "        if row[0] == '0': # find unmined txData\n",
    "            line = txData(row[0],row[1],row[2],row[3],row[4], row[5])\n",
    "            importedTx.append(line)\n",
    "    print(\"Pulling txData from csv...\")\n",
    "    return importedTx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTxData():\n",
    "    strTxData = ''\n",
    "    importedTx = readTx(db_txTableName, db_txTableColumns)\n",
    "    if len(importedTx) > 0:\n",
    "        for i in importedTx:\n",
    "            print(\"서비스 송신정보\")\n",
    "            transaction = \"[\"+ i.uuid + \"]\" \"UserKey \" + i.sender + \" sent \" + i.amount + \" bitTokens to UserKey \" + i.receiver + \". \"\n",
    "            print(transaction)\n",
    "            strTxData += transaction\n",
    "    return strTxData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minNewBlock(difficulty = g_difficulty, tableName = db_blockTableName, columns = db_blockTableColumns):\n",
    "    blockchain = readBlockchain(tableName, columns)\n",
    "    strTxData = getTxData()\n",
    "    if strTxData == '':\n",
    "        print('txdata not found, so mining aborted')\n",
    "        return\n",
    "    timestamp = time.time()\n",
    "    proof = 0\n",
    "    newBlcokFound = False\n",
    "    \n",
    "    print(\"Mining  blocks\")\n",
    "    \n",
    "    while not newBlcokFound:\n",
    "        newBlcokaAttempt = generateNextBlock(blockchain, strTxData, timestamp, proof)\n",
    "        if newBlockAttempt.currentHash[0:difficulty] == '0' * difficulty: #0부터 설정 난이도까지 0*4 = 0000이냐 로 묻는 것 - 즉 난이도를 만족하냐?\n",
    "            stopTime = time.time()\n",
    "            timer = stopTime - timestamp\n",
    "            print('New block found with proof', proof, 'in', round(timer, 2), 'seconds.')\n",
    "            newBlockFound = True\n",
    "        else:\n",
    "            proof += 1\n",
    "\n",
    "    blockchain.append(newBlockAttempt)\n",
    "    writeBlockchain(blockchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine():\n",
    "    mineNewBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSameBlock(block1, block2):\n",
    "    if str(block1.index) != str(block2.index):\n",
    "        return False\n",
    "    elif str(block1.previousHash) != str(block2.previousHash):\n",
    "        return False\n",
    "    elif str(block1.timestamp) != str(block2.timestamp):\n",
    "        return False\n",
    "    elif str(block1.data) != str(block2.data):\n",
    "        return False\n",
    "    elif str(block1.currentHash) != str(block2.currentHash):\n",
    "        return False\n",
    "    elif str(block1.proof) != str(block2.proof):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidNewBlock(newBlock, previousBlock):\n",
    "    if int(previousBlock.index) + 1 != int(newBlock.index):\n",
    "        print('Indices Do Not Match Up')\n",
    "        return False\n",
    "    elif previousBlock.currentHash != newBlock.previousHash:\n",
    "        print(\"Previous hash does not match\")\n",
    "        return False\n",
    "    elif calculateHashForBlock(newBlock) != newBlock.currentHash:\n",
    "        print(\"Hash is invalid\")\n",
    "        return False\n",
    "    elif newBlock.currentHash[0:g_difficulty] != '0' * g_difficulty:\n",
    "        print(\"Hash difficulty is invalid\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtx(txToMining, validity):\n",
    "\n",
    "    newtxData = []\n",
    "    # transform given data to txData object\n",
    "    for line in txToMining:\n",
    "        tx = txData(0, line['sender'], line['amount'], line['receiver'], uuid.uuid4(), validity)\n",
    "        newtxData.append(tx)\n",
    "\n",
    "    # limitation check : max 5 tx\n",
    "    if len(newtxData) > 5:\n",
    "        print('number of requested tx exceeds limitation')\n",
    "        return -1\n",
    "\n",
    "    if writeTx(newtxData) == 0:\n",
    "        print(\"file write error on txData\")\n",
    "        return -2\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidChain(bcToValidate):\n",
    "    genesisBlock = []\n",
    "    bcToValidateForBlock = []\n",
    "\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    blockReader = selectTable(db_blockTableName, db_blockTableColumns, engine)\n",
    "    for i in range(len(blockReader)):\n",
    "        line = blockReader.loc[i]\n",
    "        block = Block(line[0], line[1], line[2], line[3], line[4], line[5])\n",
    "        genesisBlock.append(block)\n",
    "\n",
    "    # transform given data to Block object\n",
    "    for line in bcToValidate:\n",
    "        block = Block(line['index'], line['previousHash'], line['timestamp'], line['data'], line['currentHash'], line['proof'])\n",
    "        bcToValidateForBlock.append(block)\n",
    "\n",
    "    #if it fails to read block data  from db(csv)\n",
    "    if not genesisBlock:\n",
    "        print(\"fail to read genesisBlock\")\n",
    "        return False\n",
    "\n",
    "    # compare the given data with genesisBlock\n",
    "    if not isSameBlock(bcToValidateForBlock[0], genesisBlock[0]):\n",
    "        print('Genesis Block Incorrect')\n",
    "        return False\n",
    "\n",
    "    for i in range(0, len(bcToValidateForBlock)):\n",
    "        if isSameBlock(genesisBlock[i], bcToValidateForBlock[i]) == False:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNode(queryStr):\n",
    "    # save\n",
    "    txDataList = []\n",
    "    txDataList.append([queryStr[0],queryStr[1],0]) # ip, port, # of connection fail\n",
    "    \n",
    "    nodeData = selectTable(db_nodeListTableName, db_nodeListTableColumns, engine)\n",
    "    nodeDataList = []\n",
    "    for i in range(len(nodeList)):\n",
    "        row = nodeData.loc[i]\n",
    "        if row[0] == queryStr[0] and row[1] == queryStr[1]:\n",
    "            print(\"requested node is already exists\")\n",
    "            return -1\n",
    "        else:\n",
    "            nodeDataList.append(row)\n",
    "    \n",
    "    if (len(nodeList) > 0):\n",
    "        nodeDataFrame = pd.DataFrame(nodeDataList, columns = db_nodeListTableColumns)\n",
    "    else:\n",
    "         # this is 1st time of creating node list\n",
    "        nodeDataFrame = pd.DataFrame(txDataList, columns = db_nodeListTableColumns)\n",
    "    \n",
    "    replaceTable(db_nodeListTableName, db_nodeListTableColumns)\n",
    "    try:\n",
    "        to_varchar = {c:types.VARCHAR(nodeDataFrame[c].str.len().max()) for c in nodeDataFrame.columns[nodeDataFrame.dtypes == 'object'].tolist()}\n",
    "        nodeDataFrame.to_sql(db_nodeListTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "    except:\n",
    "        print('Data save error, It seems to have an integrity or type problem.')\n",
    "        to_varchar = {c:types.VARCHAR(nodeData[c].str.len().max()) for c in nodeData.columns[nodeData.dtypes == 'object'].tolist()}\n",
    "        nodeData.to_sql(db_nodeListTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "        return 0\n",
    "    \n",
    "    print('new node written to nodelist.csv.')\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNodes(tableName, columns):\n",
    "    print(\"read Nodes\")\n",
    "    importedNodes = []\n",
    "    \n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    \n",
    "    txReader = selectTable(tableName, columns, engine)\n",
    "    for i in range(len(txReader)):\n",
    "        row = txReader.loc[i]\n",
    "        line = [row[0], row[1]]\n",
    "        importedNodes.append(line)\n",
    "    print(\"Pulling txData from csv...\")\n",
    "    return importedNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcastNewBlock(blockchain):\n",
    "    #newBlock  = getLatestBlock(blockchain) # get the latest block\n",
    "    importedNodes = readNodes(db_nodeListTableName, db_nodeListTableColumn) # get server node ip and port\n",
    "    reqHeader = {'Content-Type': 'application/json; charset=utf-8'}\n",
    "    reqBody = []\n",
    "    nodeDataList = []\n",
    "    \n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    \n",
    "    for i in blockchain:\n",
    "        reqBody.append(i.__dict__)\n",
    "\n",
    "    if len(importedNodes) > 0 :\n",
    "        for node in importedNodes:\n",
    "            try:\n",
    "                URL = \"http://\" + node[0] + \":\" + node[1] + g_receiveNewBlock  # http://ip:port/node/receiveNewBlock\n",
    "                res = requests.post(URL, headers=reqHeader, data=json.dumps(reqBody))\n",
    "                if res.status_code == 200:\n",
    "                    print(URL + \" sent ok.\")\n",
    "                    print(\"Response Message \" + res.text)\n",
    "                else:\n",
    "                    print(URL + \" responding error \" + res.status_code)\n",
    "            except:\n",
    "                print(URL + \" is not responding.\")\n",
    "                # write responding results\n",
    "                nodeData = selectTable(db_nodeListTableName, db_nodeListTableColumn, engine)\n",
    "                for i in range(len(nodeData)):\n",
    "                    row = nodeData.loc[i]\n",
    "                    if (row[0] == node[0] and row[1] == node [1]):\n",
    "                        print(\"connection failed \"+row[0]+\":\"+row[1]+\", number of fail \"+row[2])\n",
    "                        tmp = row[2]\n",
    "                        # too much fail, delete node\n",
    "                        if int(tmp) > g_maximumTry:\n",
    "                            print(row[0]+\":\"+row[1]+\" deleted from node list because of exceeding the request limit\")\n",
    "                        else:\n",
    "                            row[2] = int(tmp) + 1\n",
    "                            nodeDataList.append(row)\n",
    "                    else:\n",
    "                         nodeDataList.append(row)\n",
    "                            \n",
    "                if (len(nodeData) > 0):\n",
    "                    nodeDataFrame = pd.DataFrame(nodeDataList, columns = db_nodeListTableColumns)\n",
    "                    replaceTable(db_nodeListTableName, db_nodeListTableColumns)\n",
    "                    try:\n",
    "                        to_varchar = {c:types.VARCHAR(nodeDataFrame[c].str.len().max()) for c in nodeDataFrame.columns[nodeDataFrame.dtypes == 'object'].tolist()}\n",
    "                        nodeDataFrame.to_sql(db_nodeListTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "                    except:\n",
    "                        print('Data save error, It seems to have an integrity or type problem.')\n",
    "                        to_varchar = {c:types.VARCHAR(nodeData[c].str.len().max()) for c in nodeData.columns[nodeData.dtypes == 'object'].tolist()}\n",
    "                        nodeData.to_sql(db_nodeListTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "                else:\n",
    "                    print(\"caught exception while updating node list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareMerge(bcDict):\n",
    "\n",
    "    heldBlock = []\n",
    "    bcToValidateForBlock = []\n",
    "\n",
    "    # Read GenesisBlock\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    blockReader = selectTable(db_blockTableName, db_blockTableColumns, engine)\n",
    "    for i in range(len(blockReader)):\n",
    "        line = blockReader.loc[i]\n",
    "        block = Block(line[0], line[1], line[2], line[3], line[4], line[5])\n",
    "        heldBlock.append(block)\n",
    "    \n",
    "    if len(blockReader) == 0:\n",
    "        print(\"file open error in compareMerge or No database exists\")\n",
    "        print(\"call initSvr if this server has just installed\")\n",
    "        return -1 \n",
    "\n",
    "    #if it fails to read block data  from db(csv)\n",
    "    if len(heldBlock) == 0 :\n",
    "        print(\"fail to read\")\n",
    "        return -2\n",
    "\n",
    "    # transform given data to Block object\n",
    "    for line in bcDict:\n",
    "        # print(type(line))\n",
    "        # index, previousHash, timestamp, data, currentHash, proof\n",
    "        block = Block(line['index'], line['previousHash'], line['timestamp'], line['data'], line['currentHash'], line['proof'])\n",
    "        bcToValidateForBlock.append(block)\n",
    "\n",
    "    # compare the given data with genesisBlock\n",
    "    if not isSameBlock(bcToValidateForBlock[0], heldBlock[0]):\n",
    "        print('Genesis Block Incorrect')\n",
    "        return -1\n",
    "\n",
    "    # check if broadcasted new block,1 ahead than > last held block\n",
    "\n",
    "    if isValidNewBlock(bcToValidateForBlock[-1],heldBlock[-1]) == False:\n",
    "\n",
    "        # latest block == broadcasted last block\n",
    "        if isSameBlock(heldBlock[-1], bcToValidateForBlock[-1]) == True:\n",
    "            print('latest block == broadcasted last block, already updated')\n",
    "            return 2\n",
    "        # select longest chain\n",
    "        elif len(bcToValidateForBlock) > len(heldBlock):\n",
    "            # validation\n",
    "            if isSameBlock(heldBlock[0],bcToValidateForBlock[0]) == False:\n",
    "                    print(\"Block Information Incorrect #1\")\n",
    "                    return -1\n",
    "            tempBlocks = [bcToValidateForBlock[0]]\n",
    "            for i in range(1, len(bcToValidateForBlock)):\n",
    "                if isValidNewBlock(bcToValidateForBlock[i], tempBlocks[i - 1]):\n",
    "                    tempBlocks.append(bcToValidateForBlock[i])\n",
    "                else:\n",
    "                    return -1\n",
    "            # [START] save it to csv\n",
    "            blockchainList = []\n",
    "            for block in bcToValidateForBlock:\n",
    "                blockList = [block.index, block.previousHash, str(block.timestamp), block.data,\n",
    "                             block.currentHash, block.proof]\n",
    "                blockchainList.append(blockList)\n",
    "                \n",
    "            blockchainData = pd.DataFrame(blockchainList, columns = db_blockTableColumns)\n",
    "            blockWriter = pd.concat([blockReader, blockchainData], axis = 0).reset_index(drop = True)\n",
    "            replaceTable(db_blockTableName, db_blockTableColumns)\n",
    "            try:\n",
    "                to_varchar = {c:types.VARCHAR(blockWriter[c].str.len().max()) for c in blockWriter.columns[blockWriter.dtypes == 'object'].tolist()}\n",
    "                blockWriter.to_sql(db_blockTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "            except:\n",
    "                print('Data save error, It seems to have an integrity or type problem.')\n",
    "                to_varchar = {c:types.VARCHAR(blockReader[c].str.len().max()) for c in blockReader.columns[blockReader.dtypes == 'object'].tolist()}\n",
    "                blockReader.to_sql(db_blockTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "            # [END] save it to csv\n",
    "            return 1\n",
    "        elif len(bcToValidateForBlock) < len(heldBlock):\n",
    "            # validation\n",
    "            #for i in range(0,len(bcToValidateForBlock)):\n",
    "            #    if isSameBlock(heldBlock[i], bcToValidateForBlock[i]) == False:\n",
    "            #        print(\"Block Information Incorrect #1\")\n",
    "            #        return -1\n",
    "            tempBlocks = [bcToValidateForBlock[0]]\n",
    "            for i in range(1, len(bcToValidateForBlock)):\n",
    "                if isValidNewBlock(bcToValidateForBlock[i], tempBlocks[i - 1]):\n",
    "                    tempBlocks.append(bcToValidateForBlock[i])\n",
    "                else:\n",
    "                    return -1\n",
    "            print(\"We have a longer chain\")\n",
    "            return 3\n",
    "        else:\n",
    "            print(\"Block Information Incorrect #2\")\n",
    "            return -1\n",
    "    else: # very normal case (ex> we have index 100 and receive index 101 ...)\n",
    "        tempBlocks = [bcToValidateForBlock[0]]\n",
    "        for i in range(1, len(bcToValidateForBlock)):\n",
    "            if isValidNewBlock(bcToValidateForBlock[i], tempBlocks[i - 1]):\n",
    "                tempBlocks.append(bcToValidateForBlock[i])\n",
    "            else:\n",
    "                print(\"Block Information Incorrect #2 \"+tempBlocks.__dict__)\n",
    "                return -1\n",
    "\n",
    "        print(\"new block good\")\n",
    "\n",
    "        # validation\n",
    "        for i in range(0, len(heldBlock)):\n",
    "            if isSameBlock(heldBlock[i], bcToValidateForBlock[i]) == False:\n",
    "                print(\"Block Information Incorrect #1\")\n",
    "                return -1\n",
    "        # [START] save it to csv\n",
    "        blockchainList = []\n",
    "        for block in bcToValidateForBlock:\n",
    "            blockList = [block.index, block.previousHash, str(block.timestamp), block.data, block.currentHash, block.proof]\n",
    "            blockchainList.append(blockList)\n",
    "        blockchainData = pd.DataFrame(blockchainList, columns = db_blockTableColumns)\n",
    "        blockWriter = pd.concat([blockReader, blockchainData], axis = 0).reset_index(drop = True)\n",
    "        replaceTable(db_TxTableName, db_TxTableColumns)\n",
    "        try:\n",
    "            to_varchar = {c:types.VARCHAR(blockWriter[c].str.len().max()) for c in blockWriter.columns[blockWriter.dtypes == 'object'].tolist()}\n",
    "            blockWriter.to_sql(db_blockTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "        except:\n",
    "            print('Data save error, It seems to have an integrity or type problem.')\n",
    "            to_varchar = {c:types.VARCHAR(blockReader[c].str.len().max()) for c in blockReader.columns[blockReader.dtypes == 'object'].tolist()}\n",
    "            blockReader.to_sql(db_blockTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "        # [END] save it to csv\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initSvr():\n",
    "    print(\"init Server\")\n",
    "    connectInfo = 'oracle+cx_oracle://%s:%s@%s:%s/%s' % (db_id, db_pw, db_ip, db_port, db_serviceName)\n",
    "    engine = create_engine(connectInfo)\n",
    "    last_line_number = len(selectTable(db_nodeListTableName, db_nodeListTableColumns, engine))\n",
    "    # 1. check if we have a node list file\n",
    "    # if we don't have, let's request node list\n",
    "    if last_line_number == 0:\n",
    "        # get nodes...\n",
    "        for key, value in g_nodeList.items():\n",
    "            URL = 'http://'+key+':'+value+'/node/getNode'\n",
    "            try:\n",
    "                res = requests.get(URL)\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                continue\n",
    "            if res.status_code == 200 :\n",
    "                print(res.text)\n",
    "                tmpNodeLists = json.loads(res.text)\n",
    "                for node in tmpNodeLists:\n",
    "                    addNode(node)\n",
    "\n",
    "    # 2. check if we have a blockchain data file\n",
    "    last_line_number = len(selectTable(db_blockTableName, db_blockTableColumns, engine))\n",
    "    blockchainList=[]\n",
    "    if last_line_number == 0:\n",
    "        # get Block Data...\n",
    "        for key, value in g_nodeList.items():\n",
    "            URL = 'http://'+key+':'+value+'/block/getBlockData'\n",
    "            try:\n",
    "                res = requests.get(URL)\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                continue\n",
    "            if res.status_code == 200 :\n",
    "                print(res.text)\n",
    "                tmpbcData = json.loads(res.text)\n",
    "                for line in tmpbcData:\n",
    "                    # print(type(line))\n",
    "                    # index, previousHash, timestamp, data, currentHash, proof\n",
    "                    block = [line['index'], line['previousHash'], line['timestamp'], line['data'],line['currentHash'], line['proof']]\n",
    "                    blockchainList.append(block)\n",
    "                \n",
    "                blockchainData = pd.DataFrame(blockchainList, columns = db_blockTableColumns)\n",
    "                replaceTable(db_blockTableName, db_blockTableColumns)\n",
    "                try:\n",
    "                    to_varchar = {c:types.VARCHAR(blockchainData[c].str.len().max()) for c in blockchainData.columns[blockchainData.dtypes == 'object'].tolist()}\n",
    "                    blockchainData.to_sql(db_blockTableName, engine, if_exists = 'append', index = False, dtype = to_varchar)\n",
    "                except Exception as e:\n",
    "                    print('Data save error in initSvr()', e)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myHandler(BaseHTTPRequestHandler):\n",
    "\n",
    "    #def __init__(self, request, client_address, server):\n",
    "    #    BaseHTTPRequestHandler.__init__(self, request, client_address, server)\n",
    "\n",
    "    # Handler for the GET requests\n",
    "    def do_GET(self):\n",
    "        data = []  # response json data\n",
    "        if None != re.search('/block/*', self.path):\n",
    "            self.send_response(200)\n",
    "            self.send_header('Content-type', 'application/json')\n",
    "            self.end_headers()\n",
    "\n",
    "            # 약점 : 사이즈가 커서 한번에 주면 서버가 죽을 수 있다. 나눠서 줘야함( 페이징 처리 / 게시물의 범위 )\n",
    "            if None != re.search('/block/getBlockData', self.path):\n",
    "                # TODO: range return (~/block/getBlockData?from=1&to=300)\n",
    "                # queryString = urlparse(self.path).query.split('&')\n",
    "\n",
    "                block = readBlockchain(db_blockTableName, db_blockTableColumns, mode = 'external')\n",
    "\n",
    "                if block == None :\n",
    "                    print(\"No Block Exists\")\n",
    "                    data.append(\"no data exists\")\n",
    "                else :\n",
    "                    for i in block:\n",
    "                        print(i.__dict__)\n",
    "                        data.append(i.__dict__)\n",
    "\n",
    "                self.wfile.write(bytes(json.dumps(data, sort_keys=True, indent=4), \"utf-8\"))\n",
    "\n",
    "            elif None != re.search('/block/generateBlock', self.path):\n",
    "                t = threading.Thread(target=mine)\n",
    "                t.start()\n",
    "                data.append(\"{mining is underway:check later by calling /block/getBlockData}\")\n",
    "                self.wfile.write(bytes(json.dumps(data, sort_keys=True, indent=4), \"utf-8\"))\n",
    "            else:\n",
    "                data.append(\"{info:no such api}\")\n",
    "                self.wfile.write(bytes(json.dumps(data, sort_keys=True, indent=4), \"utf-8\"))\n",
    "\n",
    "        elif None != re.search('/node/*', self.path):\n",
    "            self.send_response(200)\n",
    "            self.send_header('Content-type', 'application/json')\n",
    "            self.end_headers()\n",
    "            if None != re.search('/node/addNode', self.path):\n",
    "                queryStr = urlparse(self.path).query.split(':')\n",
    "                print(\"client ip : \"+self.client_address[0]+\" query ip : \"+queryStr[0])\n",
    "                if self.client_address[0] != queryStr[0]:\n",
    "                    data.append(\"your ip address doesn't match with the requested parameter\")\n",
    "                else:\n",
    "                    res = addNode(queryStr)\n",
    "                    if res == 1:\n",
    "                        importedNodes = readNodes(db_nodeListTableName, db_nodeListTableColumns)\n",
    "                        data = importedNodes\n",
    "                        print(\"node added okay\")\n",
    "                    elif res == 0 :\n",
    "                        data.append(\"caught exception while saving\")\n",
    "                    elif res == -1 :\n",
    "                        importedNodes = readNodes(db_nodeListTableName, db_nodeListTableColumns)\n",
    "                        data = importedNodes\n",
    "                        data.append(\"requested node is already exists\")\n",
    "                self.wfile.write(bytes(json.dumps(data, sort_keys=True, indent=4), \"utf-8\"))\n",
    "            elif None != re.search('/node/getNode', self.path):\n",
    "                importedNodes = readNodes(db_nodeListTableName, db_nodeListTableColumns)\n",
    "                data = importedNodes\n",
    "                self.wfile.write(bytes(json.dumps(data, sort_keys=True, indent=4), \"utf-8\"))\n",
    "        else:\n",
    "            self.send_response(403)\n",
    "            self.send_header('Content-Type', 'application/json')\n",
    "            self.end_headers()\n",
    "        # ref : https://mafayyaz.wordpress.com/2013/02/08/writing-simple-http-server-in-python-with-rest-and-json/\n",
    "\n",
    "    def do_POST(self):\n",
    "        print(\"POSTPOSTPOSTPOSTPOST\")\n",
    "        if None != re.search('/block/*', self.path):\n",
    "            self.send_response(200)\n",
    "            self.send_header('Content-type', 'application/json')\n",
    "            self.end_headers()\n",
    "\n",
    "            if None != re.search('/block/validateBlock/*', self.path):\n",
    "                ctype, pdict = cgi.parse_header(self.headers['content-type'])\n",
    "                #print(ctype) #print(pdict)\n",
    "\n",
    "                if ctype == 'application/json':\n",
    "                    content_length = int(self.headers['Content-Length'])\n",
    "                    post_data = self.rfile.read(content_length)\n",
    "                    receivedData = post_data.decode('utf-8')\n",
    "                    print(type(receivedData))\n",
    "                    tempDict = json.loads(receivedData)  # load your str into a list #print(type(tempDict))\n",
    "                    if isValidChain(tempDict) == True :\n",
    "                        tempDict.append(\"validationResult:normal\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "                    else :\n",
    "                        tempDict.append(\"validationResult:abnormal\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "            elif None != re.search('/block/newtx', self.path):\n",
    "                ctype, pdict = cgi.parse_header(self.headers['content-type'])\n",
    "                if ctype == 'application/json':\n",
    "                    content_length = int(self.headers['Content-Length'])\n",
    "                    post_data = self.rfile.read(content_length)\n",
    "                    receivedData = post_data.decode('utf-8')\n",
    "                    print(type(receivedData))\n",
    "                    tempDict = json.loads(receivedData)\n",
    "                    res = newtx(tempDict)\n",
    "                    if  res == 1 :\n",
    "                        tempDict.append(\"accepted : it will be mined later\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "                    elif res == -1 :\n",
    "                        tempDict.append(\"declined : number of request txData exceeds limitation\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "                    elif res == -2 :\n",
    "                        tempDict.append(\"declined : error on data read or write\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "                    else :\n",
    "                        tempDict.append(\"error : requested data is abnormal\")\n",
    "                        self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "\n",
    "        elif None != re.search('/node/*', self.path):\n",
    "            self.send_response(200)\n",
    "            self.send_header('Content-type', 'application/json')\n",
    "            self.end_headers()\n",
    "            if None != re.search(g_receiveNewBlock, self.path): # /node/receiveNewBlock\n",
    "                content_length = int(self.headers['Content-Length'])\n",
    "                post_data = self.rfile.read(content_length)\n",
    "                receivedData = post_data.decode('utf-8')\n",
    "                tempDict = json.loads(receivedData)  # load your str into a list\n",
    "                print(tempDict)\n",
    "                res = compareMerge(tempDict)\n",
    "                if res == -1: # internal error\n",
    "                    tempDict.append(\"internal server error\")\n",
    "                elif res == -2 : # block chain info incorrect\n",
    "                    tempDict.append(\"block chain info incorrect\")\n",
    "                elif res == 1: #normal\n",
    "                    tempDict.append(\"accepted\")\n",
    "                elif res == 2: # identical\n",
    "                    tempDict.append(\"already updated\")\n",
    "                elif res == 3: # we have a longer chain\n",
    "                    tempDict.append(\"we have a longer chain\")\n",
    "                self.wfile.write(bytes(json.dumps(tempDict), \"utf-8\"))\n",
    "        else:\n",
    "            self.send_response(404)\n",
    "            self.send_header('Content-Type', 'application/json')\n",
    "            self.end_headers()\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockchain = [\n",
    "    Block(0, 0, 0, 0, 0, 0),\n",
    "    Block(1, 0, 0, 0, 0, 0),\n",
    "    Block(2, 0, 0, 0, 0, 0),\n",
    "    Block(3, 0, 0, 0, 0, 0), \n",
    "    Block(4, 0, 0, 0, 0, 0), \n",
    "    Block(5, 0, 0, 0, 0, 0),\n",
    "    Block(6, 0, 0, 0, 0, 0),\n",
    "    Block(7, 0, 0, 0, 0, 0),\n",
    "    Block(8, 0, 0, 0, 0, 0),\n",
    "    Block(9, 0, 0, 0, 0, 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFunction \"writeBlockchain\" executed\n",
      "\tFunction \"selectTable\" executed\n",
      "Index sequence mismatch\n",
      "DB has already been updated\n"
     ]
    }
   ],
   "source": [
    "writeBlockchain(blockchain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
